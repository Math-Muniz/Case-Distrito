# --- Imports B√°sicos ---
import os
import requests
import json
from dotenv import load_dotenv
from typing import Annotated

# --- Import do Streamlit ---
import streamlit as st

# --- Imports do LangChain / LangGraph ---
from langchain.tools import tool
from langgraph.graph.message import add_messages
from langgraph.graph import START, END, StateGraph
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.prebuilt import ToolNode
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage

# --- Carregamento de Vari√°veis de Ambiente ---
load_dotenv()
SPOON_API_KEY = os.getenv("SPOON_API_KEY") 
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
LANGSMITH_API_KEY = os.getenv("LANGSMITH_API_KEY")

# --- Defini√ß√£o da Ferramenta ---
@tool
def buscador_de_receitas(ingredientes: str) -> str:
    """
    Caso o input esteja em PT-BR, tranforme em EN-US (ex: 'tomatoes,cheese,eggs') para passar pela API.
    Busca receitas com base em uma lista de ingredientes dispon√≠veis.
    Verificar os ingredientes dispon√≠veis do input antes de passar pela API.
    O input deve ser uma string com os ingredientes separados por v√≠rgula (ex: 'tomatoes,cheese,eggs').
    Retorna uma lista de receitas com os ingredientes que elas usam e os que faltam.
    Sempre responda no mesmo idioma do input.
    """
    api_url = "https://api.spoonacular.com/recipes/findByIngredients"
    params = {
        'ingredients': ingredientes,
        'number': 5,
        'ranking': 1,
        'apiKey': SPOON_API_KEY
    }
    try:
        response = requests.get(api_url, params=params)
        response.raise_for_status()
        receitas = response.json()
        if not receitas:
            return "Nenhuma receita encontrada com esses ingredientes."
        
        resultado_formatado = "Encontrei estas receitas para voc√™:\n\n"
        for receita in receitas:
            resultado_formatado += f"--- T√≠tulo: {receita['title']} ---\n"
            ingredientes_usados = ', '.join([ing['name'] for ing in receita['usedIngredients']])
            resultado_formatado += f"  - Ingredientes que voc√™ tem: {ingredientes_usados}\n"
            if receita['missedIngredientCount'] > 0:
                ingredientes_faltando = ', '.join([ing['name'] for ing in receita['missedIngredients']])
                resultado_formatado += f"  - Ingredientes que faltam: {ingredientes_faltando}\n"
            else:
                resultado_formatado += "  - Voc√™ tem todos os ingredientes!\n"
            resultado_formatado += "\n"
        return resultado_formatado
    except requests.exceptions.RequestException as e:
        return f"Erro ao conectar com a API: {e}"
    except Exception as e:
        return f"Ocorreu um erro: {e}"

# --- Configura√ß√£o do Agente ---

# 1. Ferramentas
tools = [buscador_de_receitas]
tool_node = ToolNode(tools)

# 2. LLM 
llm = ChatGroq(model="llama-3.3-70b-versatile", api_key=GROQ_API_KEY, temperature=0)
llm_with_tools = llm.bind_tools(tools)

# 3. Prompt
prompt_template = ChatPromptTemplate.from_messages([
    (
        "system",
        "Voc√™ √© um assistente de culin√°ria. Seu trabalho √© usar a ferramenta 'buscador_de_receitas' para encontrar pratos para o usu√°rio. "
        "Ap√≥s receber o resultado da ferramenta, sua √∫nica tarefa √© apresentar essa informa√ß√£o de forma clara e amig√°vel para o usu√°rio. "
        "Quando for responder para o usuario sempre responda o nome da receita e os ingredientes no idioma do input."
        "Antes de dar a resposta final para o usuario confirme os ingredientes que eu disse que tinha e os que faltam para ver se est√° tudo correto"
        "N√£o chame a ferramenta novamente depois de j√° ter um resultado."
    ),
    MessagesPlaceholder(variable_name="messages"),
])

chain = prompt_template | llm_with_tools

# 4. Estado do Grafo
class State(dict):
    messages: Annotated[list, add_messages]

# 5. N√≥s do Grafo
def chatbot(state: State):
    return {"messages": [chain.invoke({"messages": state['messages']})]}

def router(state: State):
    last_message = state['messages'][-1]
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        return "tools"
    else:
        return END

# 6. Montagem do Grafo
graph_builder = StateGraph(State)
graph_builder.add_node("chatbot", chatbot)
graph_builder.add_node("tools", tool_node)
graph_builder.add_edge(START, "chatbot")
graph_builder.add_conditional_edges("chatbot", router)
graph_builder.add_edge("tools", "chatbot")

# 7. Compila√ß√£o com Mem√≥ria
memory = InMemorySaver()
graph = graph_builder.compile(checkpointer=memory)

# --- Interface com Streamlit ---
st.set_page_config(page_title="ü§ñ Chef Agente", page_icon="üßë‚Äçüç≥")

st.title("üßë‚Äçüç≥ Chef Agente")
st.caption("Um Agente de IA que mostra o que est√° fazendo por baixo dos panos.")

# Inicializa o hist√≥rico de mensagens se ele n√£o existir
if "messages" not in st.session_state:
    st.session_state.messages = []

# Exibi√ß√£o das Mensagens
for msg in st.session_state.messages:
    if isinstance(msg, HumanMessage):
        with st.chat_message("user"):
            st.markdown(msg.content)
    elif isinstance(msg, AIMessage) and msg.content:
        # Exibe a mensagem do assistente apenas se tiver conte√∫do (a resposta final)
        with st.chat_message("assistant"):
            st.markdown(msg.content)
    elif isinstance(msg, ToolMessage):
        with st.expander(f"‚öôÔ∏è Ferramenta `{msg.name}` foi usada. Clique para ver os detalhes."):
            st.code(msg.content, language=None)

# Campo de input para nova mensagem
if prompt := st.chat_input("Ingredientes Dispon√≠veis (Ex: ovos, queijo e tomate): "):
    # Adiciona a mensagem do usu√°rio ao estado e exibe na tela
    st.session_state.messages.append(HumanMessage(content=prompt))
    
    # Invoca o grafo com o hist√≥rico de mensagens
    with st.spinner("O Chef est√° pensando e usando suas ferramentas... üç≥"):
        thread_id = "conversa_principal"
        
        # O resultado do grafo cont√©m TODAS as mensagens da execu√ß√£o, incluindo as da ferramenta
        result = graph.invoke(
            {"messages": st.session_state.messages},
            config={"configurable": {"thread_id": thread_id}}
        )
        
        # Substitui o hist√≥rico antigo pelo hist√≥rico completo e atualizado da execu√ß√£o
        st.session_state.messages = result['messages']
        
        # For√ßa o Streamlit a rodar o script do in√≠cio para redesenhar a tela com as novas mensagens
        st.rerun()